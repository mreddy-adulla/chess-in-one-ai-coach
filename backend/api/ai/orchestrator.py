import redis.asyncio as redis
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from api.common.models import Game, GameState, KeyPosition, Question
from api.common.config import settings
from api.ai.validators.orchestrator_validator import validate_analyzer_output
from api.ai.providers.engine import ChessEngineProvider
from api.common.lock_manager import LockManager

class AIOrchestrator:
    """
    Implements PHASE 3: AI Orchestration Pipeline.
    Strictly follows Implementation Spec v2.0 Section 8.1.
    """
    def __init__(self, db: AsyncSession, redis_client: redis.Redis):
        self.db = db
        self.redis = redis_client
        self.lock_manager = LockManager(redis_client)
        self.engine = ChessEngineProvider()

    async def run_pipeline(self, game_id: int, tier: str = "STANDARD"):
        print(f"DEBUG: AIOrchestrator.run_pipeline started for game {game_id} with tier {tier}")
        # Transition to COACHING IMMEDIATELY if in SUBMITTED state
        # This handles cases where Redis/Lock might fail or be slow
        async with self.db.begin():
            result = await self.db.execute(select(Game).where(Game.id == game_id))
            game = result.scalar_one_or_none()
            if game and game.state == GameState.SUBMITTED:
                game.state = GameState.COACHING
                print(f"DEBUG: Game {game_id} state updated to COACHING pre-lock")
                await self.db.commit()

        # Redis lock guards AI pipeline (Implementation Spec 16)
        lock_key = f"ai_lock:{game_id}"
        try:
            async with self.lock_manager.with_lock(lock_key):
                print(f"DEBUG: Acquired lock {lock_key}")
                async with self.db.begin():
                    result = await self.db.execute(select(Game).where(Game.id == game_id))
                    game = result.scalar_one_or_none()
                    
                    if not game:
                        print(f"DEBUG: Game {game_id} not found in pipeline")
                        return
                    
                    # Ensure state is COACHING during processing
                    game.state = GameState.COACHING
                    
                    # 1. Chess Situation Analyzer (Stub - internal)
                    print(f"DEBUG: Running analyzer for game {game_id}")
                    analyzer_output = await self._run_analyzer(game, tier)
                    
                    # Validation occurs BEFORE persistence or exposure (Implementation Spec 9.3)
                    validate_analyzer_output(analyzer_output)
                    print(f"DEBUG: Analyzer output validated for game {game_id}")

                    # 2. Persist 3â€“5 key positions
                    for i, pos in enumerate(analyzer_output["key_positions"]):
                        kp = KeyPosition(
                            game_id=game_id,
                            fen=pos["fen"],
                            reason_code=pos["reason_code"],
                            engine_truth=pos["engine_truth"],
                            order=i
                        )
                        self.db.add(kp)
                        await self.db.flush()
                        print(f"DEBUG: Key position {i} persisted for game {game_id}")

                        # 3. Socratic Question Loop (Stub for questions generation)
                        await self._generate_socratic_questions(kp)
                        print(f"DEBUG: Socratic questions generated for position {i}")

                    # Update state to COACHING if it was in SUBMITTED
                    # This is redundant but ensures we are in correct state
                    result = await self.db.execute(select(Game).where(Game.id == game_id))
                    game = result.scalar_one_or_none()
                    if game:
                        game.state = GameState.COACHING
                        
                        # Use a more descriptive dynamic reflection
                        game.reflection = {
                            "thinking_patterns": [
                                f"In this session as {game.player_color}, the engine observed several critical moments.",
                                "The Socratic questions aimed to highlight where you could improve piece harmony."
                            ],
                            "missing_elements": [
                                "Long-range piece awareness",
                                "Strategic planning beyond 2 moves"
                            ],
                            "habits": [
                                "Always perform a full-board scan after every opponent move.",
                                "Practice identifying the most active piece on both sides."
                            ]
                        }
                        await self.db.commit()
                        print(f"DEBUG: Game {game_id} state set to COACHING and dynamic reflection persisted")
        except RuntimeError as e:
            if "Could not acquire lock" in str(e):
                print(f"DEBUG: Could not acquire lock for game {game_id}")
                # State already set to COACHING, so user won't hang
                return
            raise
        except redis.ConnectionError as e:
            print(f"CRITICAL: Redis connection failed during pipeline for game {game_id}: {str(e)}")
            # Fallback: We already set state to COACHING, so the user won't hang forever
            # but we need to run analyzer without lock if absolutely necessary?
            # For now, let's just log it. The state transition above prevents total hang.
            pass
        except Exception as e:
            print(f"ERROR: Unexpected error in pipeline for game {game_id}: {str(e)}")
            import traceback
            traceback.print_exc()

    async def _run_analyzer(self, game: Game, tier: str = "STANDARD"):
        """
        Internal Analyzer - Produces 3-5 key positions.
        Implementation Spec 10.1: JSON output only, no language.
        """
        # In a real implementation, 'tier' would influence model selection
        # For v2.0 scaffolding, we just acknowledge it.
        
        # Mocking output for Phase 3
        return {
            "key_positions": [
                {
                    "fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3",
                    "reason_code": "THREAT_AWARENESS",
                    "engine_truth": {
                        "score": 0.5,
                        "best_move": "Bb5",
                        "threats": ["Nxe5"]
                    }
                }
            ]
        }

    async def _generate_socratic_questions(self, kp: KeyPosition):
        """
        Socratic Questioner - Implementation Spec 12
        """
        categories = ["OPP_INTENT", "THREAT", "CHANGE", "WORST_PIECE", "ALTERNATIVES", "REFLECTION"]
        for i, cat in enumerate(categories):
            q = Question(
                key_position_id=kp.id,
                category=cat,
                question_text=f"Stub question for {cat}",
                order=i
            )
            self.db.add(q)
